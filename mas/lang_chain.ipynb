{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GPT4AllEmbeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../dataset/auto_1_langchain.csv')\n",
    "names = df['name'].to_list()\n",
    "steps = df['steps'].to_list()\n",
    "ingredients = df['ingredients'].to_list()\n",
    "\n",
    "texts = []\n",
    "\n",
    "for n, s, i in zip(names, steps, ingredients):\n",
    "    texts.append(f'name: {n} ; ingredients: {i} ; preparation: {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maksim\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 76.12it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "tmp = []\n",
    "\n",
    "for i in tqdm(range(1000)):\n",
    "    examples = random.choices(texts, k=2)\n",
    "    tmp.append(sentence_bleu(examples[0], examples[1], weights=[0.5, 0.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865476\n"
     ]
    }
   ],
   "source": [
    "sentences = [\n",
    "    \"Preheat oven to 350 degrees F. Spray an 11x7x2-inch baking dish with cooking spray. In a large bowl, combine the chicken, celery and water chestnuts. Stir in the soup and mayonnaise. Spread half the mixture into the prepared pan. Unroll the dough and separate into 9 squares. Place 3 squares in the bottom of the pan. Spread with half the sour cream. Top with half the chicken mixture and half the Parmesan cheese. Repeat the layers. Bake for 40 minutes. Let stand for 10 minutes. Cut into 6 squares. Serve warm.\", \n",
    "    \"Preheat the oven to 375 degrees. Put the chicken, celery, water chestnuts, soup, mayonnaise and sour cream in a large bowl and stir until well combined. Put the chicken mixture into a 9x13 baking dish that has been coated with the cooking spray. Bake for 25-30 minutes, or until celery is crisp-tender. Remove the casserole from the oven. Unroll the croissant dough into flat pieces and cover the chicken mixture with the slices of dough. Drizzle with the melted butter. Bake for 10-12 minutes or until the croissant topping is turning golden brown. Remove the casserole from the oven and sprinkle with the parmesan cheese. Bake for 1-2 more minutes. Let stand 5 minutes before serving.\"\n",
    "    ]\n",
    "\n",
    "\n",
    "print(sentence_bleu([sentences[0]], sentences[1], weights=[0.5, 0.5])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.768142374145008e-232, 1.0365786590631523e-231, 9.935665127248702e-232, 1.0400016401250536e-231, 1.1660767189343928e-231, 1.0430324475818594e-231, 9.53667732973359e-232, 1.0126455930533508e-231, 9.888463483086503e-232, 1.0414710173847794e-231, 8.924178303539991e-232, 1.0179167992307073e-231, 9.94262531724427e-232, 1.0881585058887664e-231, 1.0049683504181483e-231, 9.844849180073599e-232, 1.0013597882056462e-231, 8.850193570579603e-232, 1.0572155961960705e-231, 9.115950231440499e-232, 9.817780325683613e-232, 9.718756419071616e-232, 1.0377133938315695e-231, 1.072756664658447e-231, 1.015383921264607e-231, 9.468085247078445e-232, 9.187929551544542e-232, 9.971469906067022e-232, 9.13647407667802e-232, 1.0505112911309325e-231, 9.96763840875745e-232, 1.0477021619035363e-231, 9.884192426889356e-232, 1.0649365332831978e-231, 1.137030182511939e-231, 9.500581443932479e-232, 9.379965251837572e-232, 1.1378719080845818e-231, 1.0296788793467617e-231, 9.514993728653354e-232, 1.0865159978022652e-231, 1.0787001422877275e-231, 9.445485834201323e-232, 1.0732031550278091e-231, 1.0045458284200586e-231, 1.0023556718711968e-231, 1.0187517416993229e-231, 1.0003688322288243e-231, 9.426408193243007e-232, 9.477517004059421e-232, 1.0126455930533508e-231, 9.137850569985851e-232, 9.578525475899157e-232, 1.0093095674422238e-231, 9.788429383461836e-232, 1.1217867005881972e-231, 1.0340225941931413e-231, 1.0423153399406431e-231, 9.79865230014291e-232, 1.1001549658594769e-231, 1.1070900996592687e-231, 1.0618063059647207e-231, 9.984015290369887e-232, 9.788429383461836e-232, 9.55644398278701e-232, 1.0572155961960705e-231, 9.654125561381907e-232, 1.0832677820940877e-231, 9.594503055152632e-232, 9.466071831707777e-232, 9.99235267992186e-232, 1.0023556718711968e-231, 9.654125561381907e-232, 9.65701126654974e-232, 9.191432333282115e-232, 9.036377871887874e-232, 9.81643662068717e-232, 9.31870284613609e-232, 1.0643741755154884e-231, 1.0832677820940877e-231, 9.546964831161274e-232, 9.012753152467098e-232, 1.0933726393906153e-231, 9.645494237992062e-232, 1.035450101419401e-231, 9.269231506606651e-232, 9.951898722614107e-232, 1.0063893777936914e-231, 9.425708930733317e-232, 9.577802377303848e-232, 9.336117803135294e-232, 9.5079272598147e-232, 9.475293539507031e-232, 1.0221761655852355e-231, 9.763103351382646e-232, 1.1008876702055895e-231, 1.0707423501444129e-231, 1.044014311087766e-231, 1.0221761655852355e-231, 9.868472263338009e-232, 9.243956941585312e-232, 1.0505112911309325e-231, 9.109159947227211e-232, 1.0223681055364786e-231, 1.0051623035972895e-231, 9.52208653546315e-232, 1.0760311193429945e-231, 1.0662922795459185e-231, 9.19946383953916e-232, 1.0299555632032392e-231, 9.122765899925687e-232, 1.060920359172574e-231, 1.0662922795459185e-231, 1.0071121911378698e-231, 1.0782486457916595e-231, 9.358197857531086e-232, 1.0969346883576932e-231, 9.539155446441393e-232, 9.697132659139968e-232, 9.159350055604075e-232, 9.916649413060699e-232, 9.122145240269897e-232, 1.0049683504181483e-231, 9.082249284095857e-232, 9.418382295637229e-232, 9.966150368347767e-232, 9.884192426889356e-232, 1.0115896764576124e-231, 9.77874265276164e-232, 1.1134712866512498e-231, 1.0189979657032352e-231, 1.0003688322288243e-231, 9.684433249879153e-232, 9.360774808858168e-232, 1.077698097414162e-231, 9.984831144536788e-232, 9.200354095884341e-232, 1.0088115117427719e-231, 1.072268715173723e-231, 9.350808828590385e-232, 1.0701348365097317e-231, 9.83138096071417e-232, 1.0335541382244337e-231, 9.472983900447304e-232, 9.80794778423036e-232, 1.0452976746795178e-231, 9.860673615422575e-232, 1.0364758085723659e-231, 9.842362164798581e-232, 1.009468626554716e-231, 1.0176229291719042e-231, 1.0036434057440511e-231, 1.0589666839288034e-231, 1.0181865532756278e-231, 9.145094214651507e-232, 9.367879766389286e-232, 1.0545082357000832e-231, 1.1527770541523282e-231, 1.0400016401250536e-231, 9.528557546016516e-232, 9.884192426889356e-232, 9.455303357414031e-232, 9.854411939457176e-232, 1.1000156792876774e-231, 9.153488068566317e-232, 9.80705635520132e-232, 9.479921278878616e-232, 9.458857582510812e-232, 1.0423153399406431e-231, 1.0409900793588983e-231, 1.0473875838281341e-231, 9.893133360884868e-232, 1.0319427527676151e-231, 1.013799606997597e-231, 9.80549732862702e-232, 1.1042491983919341e-231, 9.5236319524631e-232, 9.103239138153915e-232, 1.1400331180262983e-231, 1.0202666183615141e-231, 1.0003688322288243e-231, 1.0144951817511773e-231, 1.125562866780732e-231, 9.445126669768344e-232, 1.0319427527676151e-231, 1.0025459263491045e-231, 9.675249329403791e-232, 9.920201010148415e-232, 9.788429383461836e-232, 1.1165890934539582e-231, 1.0692267874642598e-231, 9.240260688720815e-232, 1.0244914152188952e-231, 1.0714126117246195e-231, 1.0904373843502162e-231, 1.0531674723587529e-231, 1.0377133938315695e-231, 1.157805223022965e-231, 1.0684046209298366e-231, 1.0003688322288243e-231, 1.0033565275081439e-231, 1.1070900996592687e-231, 1.0344876370783809e-231, 9.55644398278701e-232, 9.455303357414031e-232, 1.046588965188974e-231, 9.633332270395829e-232, 9.635038402313744e-232, 1.0187517416993229e-231, 1.0377133938315695e-231, 1.0757078800624923e-231, 1.0732031550278091e-231, 1.0233808537870939e-231, 1.0266369587288231e-231, 9.66019052994978e-232, 1.1420323866536e-231, 1.0531674723587529e-231, 1.072268715173723e-231, 9.835332640021048e-232, 9.87915668361586e-232, 9.466071831707777e-232, 9.403813988622147e-232, 9.573330797292281e-232, 9.257324954728539e-232, 1.1181439502945634e-231, 1.0763278491286425e-231, 9.907209469556489e-232, 1.0114981174507752e-231, 1.046342389847311e-231, 9.005433441115789e-232, 1.0423153399406431e-231, 9.618721597958482e-232, 9.354783909912307e-232, 9.39332207632997e-232, 1.0470211721632915e-231, 9.701720847155801e-232, 1.093822785551272e-231, 9.788429383461836e-232, 9.875291749312237e-232, 9.418382295637229e-232, 9.418382295637229e-232, 1.0024464526239052e-231, 9.342836855920344e-232, 9.586830515586928e-232, 1.0468143531561773e-231, 9.52208653546315e-232, 1.0109786810769141e-231, 9.557951902988592e-232, 9.609320864633963e-232, 9.618721597958482e-232, 1.0449913962105771e-231, 1.1042491983919341e-231, 9.902114880221486e-232, 9.50440384721771e-232, 9.500581443932479e-232, 1.0746124859101448e-231, 9.693258806094018e-232, 1.0286308587733217e-231, 1.0192239326681782e-231, 9.249992414358479e-232, 9.939763718065626e-232, 9.577010725242957e-232, 1.0518351895246305e-231, 1.012071042130996e-231, 8.996292404020604e-232, 9.6929210913015e-232, 1.0211776529492471e-231, 1.0157182040823076e-231, 9.740468953067304e-232, 9.840085157783916e-232, 9.60220639580595e-232, 1.0063893777936914e-231, 9.578525475899157e-232, 9.926134502249994e-232, 1.0049683504181483e-231, 1.0244914152188952e-231, 1.0419111066516066e-231, 1.0663466891533755e-231, 9.637560994074073e-232, 9.539155446441393e-232, 9.635038402313744e-232, 9.95267346075564e-232, 9.511751671682387e-232, 1.0147073526676296e-231, 9.860673615422575e-232, 1.0863672923997675e-231, 1.0383079018919809e-231, 9.636901361545924e-232, 1.0662922795459185e-231, 8.972141065609098e-232, 1.0601790118256487e-231, 9.278484692654553e-232, 1.1025620289143715e-231, 1.0518351895246305e-231, 1.0480583415103432e-231, 9.91278218331458e-232, 1.0100728664580849e-231, 1.1152913275191404e-231, 9.65701126654974e-232, 9.724239499626535e-232, 9.539155446441393e-232, 1.075292262203214e-231, 1.1293210588544381e-231, 1.0556810861787829e-231, 1.0088115117427719e-231, 9.868472263338009e-232, 1.0622506694459948e-231, 9.994237496129991e-232, 1.0395020475131661e-231, 1.0131693302820711e-231, 9.587681505832515e-232, 9.54142988460631e-232, 9.808928824281052e-232, 9.718756419071616e-232, 9.869893516672115e-232, 1.0580118213205114e-231, 9.509041274236082e-232, 1.0395020475131661e-231, 9.854411939457176e-232, 9.257324954728539e-232, 1.0059010366659207e-231, 1.0106303679567871e-231, 1.0531674723587529e-231, 9.325649330904283e-232, 9.148396289287248e-232, 9.628346624876333e-232, 9.693258806094018e-232, 9.31610457379571e-232, 1.0739912407500404e-231, 9.480553210441019e-232, 1.0256589286974838e-231, 9.67150479564561e-232, 1.0109786810769141e-231, 9.637560994074073e-232, 9.788429383461836e-232, 9.579188556134313e-232, 9.409991746288244e-232, 9.570586367980346e-232, 9.565206520660936e-232, 9.780289310406184e-232, 1.012071042130996e-231, 8.949340014557711e-232, 1.0244914152188952e-231, 9.263764836852343e-232, 9.94524168306446e-232, 1.0013597882056462e-231, 9.159350055604075e-232, 1.0244914152188952e-231, 1.0443483422651562e-231, 1.0592691162555667e-231, 1.0491956826752202e-231, 1.1182298193540246e-231, 1.0071121911378698e-231, 9.68098417237491e-232, 9.418382295637229e-232, 9.67150479564561e-232, 1.095939735235692e-231, 9.844849180073599e-232, 9.883381615806163e-232, 9.866431003065034e-232, 9.926134502249994e-232, 1.0210282960987153e-231, 1.0688646445546163e-231, 9.779950874525103e-232, 9.175029456878946e-232, 1.0787001422877275e-231, 1.0154730095673908e-231, 9.578525475899157e-232, 9.984831144536788e-232, 9.497084316300025e-232, 1.0556810861787829e-231, 9.036094799512461e-232, 1.095939735235692e-231, 9.654125561381907e-232, 9.727330539821454e-232, 1.0656698251147616e-231, 9.744592951176694e-232, 9.033138058565268e-232, 1.060533917453014e-231, 1.101763128289178e-231, 1.0041944042528399e-231, 1.0681838077277691e-231, 1.0676566998408134e-231, 1.0074105758376563e-231, 1.0196701453686696e-231, 1.0208648400306426e-231, 9.985582024868538e-232, 1.111129588546017e-231, 1.0288051586793896e-231, 1.0096755909175496e-231, 9.5236319524631e-232, 1.0003688322288243e-231, 9.729838461201481e-232, 1.0749662678465409e-231, 1.0480583415103432e-231, 9.676444763952159e-232, 9.585736939966629e-232, 9.676444763952159e-232, 1.005685417125064e-231, 9.87783071127706e-232, 9.769977973657288e-232, 1.016500342875109e-231, 9.873677743905318e-232, 9.659775546571729e-232, 1.0323224925752969e-231, 1.072756664658447e-231, 1.030263426206844e-231, 1.06326986051457e-231, 9.515917379950267e-232, 9.594503055152632e-232, 9.697869887461275e-232, 9.929306298309508e-232, 1.0033565275081439e-231, 9.497084316300025e-232, 9.545460542950439e-232, 1.0359400235349853e-231, 9.710219953726468e-232, 1.1872004050154218e-231, 1.0084369761253534e-231, 1.0419111066516066e-231, 9.814087347664602e-232, 1.0140891399720967e-231, 8.998725478591145e-232, 1.0443483422651562e-231, 1.1160759156423089e-231, 9.326484650700517e-232, 1.0221761655852355e-231, 1.1477625987309588e-231, 1.0647435575512163e-231, 9.79694477157727e-232, 9.641193013181824e-232, 9.619829724349134e-232, 9.897814286424508e-232, 1.1200407237786664e-231, 1.015383921264607e-231, 9.729838461201481e-232, 9.753281852319978e-232, 9.888463483086503e-232, 1.0530561262164382e-231, 1.044014311087766e-231, 1.1400331180262983e-231, 1.040210917052894e-231, 1.0109786810769141e-231, 9.039352811507815e-232, 9.46481779818571e-232, 9.854411939457176e-232, 9.132076253946826e-232, 9.485368294342529e-232, 9.553021309704285e-232, 9.66019052994978e-232, 1.0013597882056462e-231, 9.586830515586928e-232, 1.1121803032788522e-231, 1.0328553043617714e-231, 1.0040399721984063e-231, 9.814087347664602e-232, 9.568324085262966e-232, 9.863250847614406e-232, 1.0024464526239052e-231, 9.610614787893563e-232, 9.729838461201481e-232, 8.991089726719026e-232, 9.455303357414031e-232, 1.0043623999766922e-231, 9.432450120194405e-232, 9.812101695090082e-232, 1.072268715173723e-231, 9.893133360884868e-232, 9.403154465246745e-232, 1.0244914152188952e-231, 1.0003688322288243e-231, 1.0415721556250988e-231, 9.80549732862702e-232, 9.788429383461836e-232, 1.0690298945754517e-231, 9.469524712759822e-232, 1.1358198313700678e-231, 9.947643045471176e-232, 9.602908234526788e-232, 1.0179167992307073e-231, 1.0157715464244269e-231, 1.0244914152188952e-231, 1.0045458284200586e-231, 1.016127520387007e-231, 1.0051623035972895e-231, 1.0083974024188332e-231, 9.854411939457176e-232, 1.0033565275081439e-231, 9.724598307793293e-232, 9.964534192121423e-232, 1.0303965093880177e-231, 9.500886953425176e-232, 1.072268715173723e-231, 1.0196701453686696e-231, 1.0589666839288034e-231, 1.1758902913522369e-231, 1.0383079018919809e-231, 1.0555194322292989e-231, 1.0184350429959432e-231, 9.43136424553e-232, 1.0543913084670045e-231, 9.500581443932479e-232, 8.996292404020604e-232, 1.1344031528025553e-231, 9.706738547745274e-232, 1.0003688322288243e-231, 1.0638581151575014e-231, 1.0774591664389445e-231, 9.409991746288244e-232, 1.0257760464760049e-231, 9.769977973657288e-232, 1.0383079018919809e-231, 9.98108097255302e-232, 1.0596677124186931e-231, 9.649085932614922e-232, 9.163329787177458e-232, 9.788429383461836e-232, 9.493872635648454e-232, 9.69875493149427e-232, 9.46277495815272e-232, 9.500581443932479e-232, 1.1070900996592687e-231, 9.935665127248702e-232, 1.0233808537870939e-231, 1.0233808537870939e-231, 1.0592691162555667e-231, 1.0818009300065748e-231, 1.0599580721425215e-231, 1.147212960250001e-231, 1.0171544317735793e-231, 1.0622506694459948e-231, 9.324655390114502e-232, 9.080914150023978e-232, 1.0312839954879021e-231, 9.62686271535747e-232, 1.0704119658444467e-231, 1.0335989011748038e-231, 9.993827602250699e-232, 1.1792950038998509e-231, 9.356347862806685e-232, 9.875291749312237e-232, 9.91475782510344e-232, 9.883381615806163e-232, 9.740468953067304e-232, 9.902933405946996e-232, 1.0147073526676296e-231, 1.0530071525561705e-231, 1.0204868118821698e-231, 1.1293210588544381e-231, 9.469524712759822e-232, 1.0470211721632915e-231, 9.65701126654974e-232, 9.91278218331458e-232, 9.744592951176694e-232, 9.526532993147009e-232, 9.788429383461836e-232, 1.0063893777936914e-231, 1.0804649959141085e-231, 9.79731584688699e-232, 9.659775546571729e-232, 9.355379547360902e-232, 1.0577612560403046e-231, 9.788429383461836e-232, 9.243956941585312e-232, 1.0395020475131661e-231, 9.884192426889356e-232, 9.26435139238975e-232, 9.161131906599197e-232, 9.455303357414031e-232, 1.1274341332018836e-231, 9.772182971647901e-232, 1.0732031550278091e-231, 9.984831144536788e-232, 9.947643045471176e-232, 8.96615364748313e-232, 9.863250847614406e-232, 9.763103351382646e-232, 9.80705635520132e-232, 9.646872658068961e-232, 9.533966891110756e-232, 9.035752192266179e-232, 9.036377871887874e-232, 9.897814286424508e-232, 1.0232753995269474e-231, 9.285591924746308e-232, 1.0234270071167476e-231, 9.929306298309508e-232, 9.825861918744841e-232, 9.764110116371739e-232, 9.922680130923814e-232, 1.0546363732849673e-231, 1.0556810861787829e-231, 9.77874265276164e-232, 1.1200407237786664e-231, 1.0518351895246305e-231, 9.520472212011834e-232, 9.579798792707353e-232, 9.425708930733317e-232, 9.788429383461836e-232, 1.0328068100871131e-231, 1.1545437674642806e-231, 9.529205857470776e-232, 1.0483580008236357e-231, 9.916649413060699e-232, 1.0847446332149227e-231, 9.906079691777987e-232, 8.972141065609098e-232, 1.0370642140370256e-231, 9.475293539507031e-232, 9.418382295637229e-232, 1.0244914152188952e-231, 1.0306819773498303e-231, 1.0746124859101448e-231, 9.102394877617175e-232, 9.508233952305399e-232, 1.0787001422877275e-231, 9.871742677130652e-232, 9.200354095884341e-232, 9.231787263313528e-232, 9.684833540589134e-232, 9.23640431265022e-232, 1.1018554844765736e-231, 1.0154730095673908e-231, 9.724598307793293e-232, 1.1816700046681467e-231, 9.848828023055687e-232, 1.0278596516071025e-231, 1.103803954481326e-231, 1.0319427527676151e-231, 9.907209469556489e-232, 9.257324954728539e-232, 1.0736132553884534e-231, 1.045198674093653e-231, 9.825861918744841e-232, 1.0414710173847794e-231, 1.0234270071167476e-231, 9.250907394577878e-232, 9.035752192266179e-232, 9.232802024226747e-232, 1.0077365911842547e-231, 1.0455777126596572e-231, 9.594503055152632e-232, 1.0073082227171603e-231, 1.0531093713753305e-231, 9.159350055604075e-232, 9.365616173190982e-232, 1.0760311193429945e-231, 1.0164555680553985e-231, 9.536351873634316e-232, 9.651494548351093e-232, 1.0244914152188952e-231, 1.0389584031415276e-231, 9.169448875586943e-232, 9.629973172213905e-232, 1.0494143790253504e-231, 1.0040399721984063e-231, 1.1121803032788522e-231, 9.563995505062079e-232, 1.0221761655852355e-231, 1.1477625987309588e-231, 9.13408264234744e-232, 9.536351873634316e-232, 1.0164555680553985e-231, 9.788429383461836e-232, 9.762009630047334e-232, 9.866431003065034e-232, 9.624253809233403e-232, 1.0210282960987153e-231, 1.1064619631796825e-231, 1.0003688322288243e-231, 9.220661561382961e-232, 9.79694477157727e-232, 1.1237070272178585e-231, 9.343448878907464e-232, 9.972923155381431e-232, 1.0975766923518679e-231, 9.609320864633963e-232, 1.060920359172574e-231, 9.228167663612764e-232, 1.0518351895246305e-231, 9.935665127248702e-232, 1.060533917453014e-231, 1.0068380670592587e-231, 1.0530071525561705e-231, 9.347651015737196e-232, 9.56124604509525e-232, 1.0904373843502162e-231, 1.1923649408980313e-231, 9.672960142131617e-232, 1.010505575663513e-231, 9.971469906067022e-232, 9.641193013181824e-232, 1.001280330971317e-231, 9.257324954728539e-232, 1.0580118213205114e-231, 1.0565626577913395e-231, 9.48688413430962e-232, 9.675249329403791e-232, 9.361047578577237e-232, 9.163329787177458e-232, 9.847576663138791e-232, 1.1238831154513378e-231, 1.0407030917432199e-231, 1.032438204339929e-231, 9.960437468111093e-232, 1.0366657262732162e-231, 9.56124604509525e-232, 9.411387576891149e-232, 1.0136487102743443e-231, 1.0347661519348002e-231, 1.0847446332149227e-231, 9.65701126654974e-232, 9.702932729116586e-232, 1.0135220437664235e-231, 9.257324954728539e-232, 1.0098921897911125e-231, 9.309480961664223e-232, 9.869893516672115e-232, 1.0396732012273364e-231, 1.1640469867513693e-231, 9.440448160942182e-232, 9.825861918744841e-232, 9.684833540589134e-232, 9.907209469556489e-232, 9.848828023055687e-232, 9.220949693551428e-232, 9.712249045875841e-232, 1.025608028772767e-231, 9.403154465246745e-232, 9.28844245215026e-232, 1.0132794873187836e-231, 9.893133360884868e-232, 1.0414710173847794e-231, 1.0669733992029681e-231, 9.386616641529765e-232, 9.693660924233881e-232, 1.1008876702055895e-231, 9.55644398278701e-232, 9.601897668005704e-232, 1.0371679463928142e-231, 1.0021959975256723e-231, 9.81305455761118e-232, 9.81305455761118e-232, 9.875291749312237e-232, 9.718328703968335e-232, 1.0321008928002798e-231, 1.1183801695853451e-231, 9.966150368347767e-232, 1.0562640672124317e-231, 1.0377133938315695e-231, 1.0846261142693498e-231, 9.249992414358479e-232, 1.0343276606355068e-231, 9.46481779818571e-232, 8.648001017827872e-232, 9.350808828590385e-232, 1.0531674723587529e-231, 1.0332112772921225e-231, 9.579798792707353e-232, 1.0975766923518679e-231, 9.746399948297577e-232, 1.0580118213205114e-231, 9.96763840875745e-232, 9.539155446441393e-232, 1.0817344905203187e-231, 1.1503153024161147e-231, 1.1409851298103347e-231, 1.0359400235349853e-231, 9.712249045875841e-232, 1.016127520387007e-231, 9.925659169578345e-232, 1.0264711180063648e-231, 1.0505686788720825e-231, 9.911137317717096e-232, 1.1600398808685376e-231, 1.0267307540757055e-231, 9.360774808858168e-232, 9.54142988460631e-232, 9.382171005136092e-232, 9.439523578573714e-232, 9.470276172887311e-232, 9.676444763952159e-232, 9.651494548351093e-232, 1.0328068100871131e-231, 1.0053733347238304e-231, 9.524821588605612e-232, 9.888019307296994e-232, 9.824219484601422e-232, 9.515917379950267e-232, 1.1083517428455008e-231, 9.512421065519574e-232, 1.0224526162300624e-231, 9.754733773737682e-232, 1.0149602266506081e-231, 1.0787001422877275e-231, 1.0136487102743443e-231, 1.1293210588544381e-231, 9.867694391400697e-232, 9.324655390114502e-232, 1.0296788793467617e-231, 9.586830515586928e-232, 1.1025620289143715e-231, 9.322751816393185e-232, 1.0014544212727645e-231, 1.1258292406880784e-231, 9.713416734391049e-232, 8.910124435838328e-232, 9.882409081318387e-232, 1.0281833359965572e-231, 1.0340225941931413e-231, 9.577802377303848e-232, 9.397475657746044e-232, 9.857609866842017e-232, 1.0084369761253534e-231, 1.0846828965177167e-231, 9.932627692417928e-232, 9.947643045471176e-232, 9.746399948297577e-232, 9.455303357414031e-232, 9.317052729582789e-232, 9.610614787893563e-232, 1.0847446332149227e-231, 1.0164555680553985e-231, 9.371870344404295e-232, 9.808928824281052e-232, 9.567024482640688e-232, 1.0530561262164382e-231, 9.46277495815272e-232, 9.317052729582789e-232, 9.788429383461836e-232, 9.489792883427431e-232, 9.647956393709427e-232, 9.433734232963385e-232, 1.0815817570903536e-231, 1.1491065328840764e-231, 9.715647908705012e-232, 9.83138096071417e-232, 1.095939735235692e-231, 1.1065437930041039e-231, 9.68098417237491e-232, 9.66610879078403e-232, 9.96763840875745e-232, 9.520472212011834e-232, 1.0494143790253504e-231, 9.265055504224164e-232, 9.866431003065034e-232, 9.769977973657288e-232, 9.544831828273173e-232, 1.174376721100895e-231, 1.1325570878536863e-231, 9.204423789494686e-232, 1.0151057719653971e-231, 1.0114981174507752e-231, 1.0074105758376563e-231, 1.1330414385984988e-231, 9.788429383461836e-232, 1.0505112911309325e-231, 9.619829724349134e-232, 9.102701836151704e-232, 9.151152577837433e-232, 1.0518351895246305e-231, 9.705257620317487e-232, 1.0482143606201075e-231, 9.38504236614691e-232, 1.1143980217744192e-231, 9.947643045471176e-232, 1.0335989011748038e-231, 1.0142736036705587e-231, 9.779181919777936e-232, 1.0033565275081439e-231, 9.403154465246745e-232, 9.669759824028608e-232, 9.814087347664602e-232, 9.515917379950267e-232, 9.517991306990327e-232, 1.0084884335551695e-231, 1.092283270426751e-231, 1.0286308587733217e-231, 1.0164555680553985e-231, 9.559344885886902e-232, 1.091900080306308e-231, 9.480553210441019e-232, 1.0669733992029681e-231, 9.311782155996212e-232, 9.780289310406184e-232, 9.515917379950267e-232, 9.814087347664602e-232, 9.851496617473517e-232, 1.0192239326681782e-231, 1.0003688322288243e-231, 9.684433249879153e-232, 1.0948551819675659e-231, 9.449212109829722e-232, 9.492959631585355e-232, 9.5236319524631e-232, 1.0474976324332058e-231, 1.0301362096301137e-231, 1.0493097566842535e-231, 9.94524168306446e-232, 9.779583075841716e-232, 9.954864569678793e-232, 9.848828023055687e-232, 1.0068380670592587e-231, 9.929306298309508e-232, 9.058629530165363e-232, 9.488470172003636e-232, 9.6929210913015e-232, 1.0022724922785631e-231, 1.0592691162555667e-231, 8.88263220926121e-232, 1.0377133938315695e-231, 8.934065039934914e-232, 9.342836855920344e-232, 9.155283750470673e-232, 9.388048947460503e-232, 1.0244914152188952e-231, 1.0382602785504863e-231, 1.0142736036705587e-231, 9.888019307296994e-232, 9.812101695090082e-232, 1.1070900996592687e-231, 1.0873685536398248e-231, 9.643248923952956e-232, 9.087934575947136e-232, 1.0157715464244269e-231, 9.438673362525564e-232, 8.755037397750627e-232, 1.1758902913522369e-231, 9.322751816393185e-232, 1.0036434057440511e-231, 1.0149602266506081e-231, 1.0503030147519885e-231, 1.0087253069557242e-231, 8.966651836324538e-232, 1.0176229291719042e-231, 1.0210282960987153e-231, 9.982151731671306e-232, 1.0470211721632915e-231, 9.65701126654974e-232, 1.1429886232668031e-231, 9.697132659139968e-232, 9.222587663185988e-232, 1.0618063059647207e-231, 1.0580118213205114e-231, 1.0787001422877275e-231, 9.328815398015602e-232, 9.669759824028608e-232, 1.0025459263491045e-231, 9.528557546016516e-232, 1.06326986051457e-231, 1.0470211721632915e-231, 9.95687520672521e-232, 9.668092192713454e-232, 9.496420300612932e-232, 9.87915668361586e-232, 9.208873740252323e-232, 1.0352455680561202e-231, 9.121059593688281e-232, 9.544831828273173e-232, 9.641193013181824e-232, 9.848828023055687e-232, 1.060533917453014e-231, 9.884192426889356e-232, 9.982151731671306e-232, 1.0850568348973216e-231, 9.281215024201387e-232, 1.001404948410034e-231, 9.962772584311345e-232, 9.74262359052368e-232, 1.1253199307903996e-231, 9.788429383461836e-232, 1.0518351895246305e-231, 9.619829724349134e-232, 9.175029456878946e-232, 1.0033565275081439e-231, 9.53667732973359e-232, 9.779583075841716e-232, 9.075583408572509e-232, 9.762009630047334e-232, 9.249626548940536e-232, 9.339620380304613e-232, 1.0350003944289303e-231, 1.009468626554716e-231, 1.0109786810769141e-231, 9.301939861137233e-232, 9.570586367980346e-232, 1.012071042130996e-231, 1.0347661519348002e-231, 1.0480583415103432e-231, 1.1165890934539582e-231, 1.1086682791458533e-231, 1.0407030917432199e-231, 1.035771919671626e-231, 9.821332131024636e-232, 1.013799606997597e-231, 9.857609866842017e-232, 1.0198868428935033e-231, 1.1461056105424777e-231, 1.0662922795459185e-231, 1.1217867005881972e-231]\n"
     ]
    }
   ],
   "source": [
    "print(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tmp = []\n",
    "for i in tmp:\n",
    "    new_tmp.append(i[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.543106092879663e-155\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "print(statistics.mean(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.69976513]]\n",
      "[[0.30023487]]\n"
     ]
    }
   ],
   "source": [
    "print(cosine_distances(X, Y))\n",
    "print(cosine_similarity(X, Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question, 5)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b.Q6_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 18\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q6_K:  226 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q6_K\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 5.15 GiB (6.56 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   102.54 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  5169.81 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    64.03 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   656.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    32.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '18'}\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1  # Metal set to 1 is enough.\n",
    "n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/llama-2-13b-chat.ggufv3.q4_0.bin\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =      67.02 ms\n",
      "llama_print_timings:      sample time =      18.38 ms /   110 runs   (    0.17 ms per token,  5985.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =      66.99 ms /     6 tokens (   11.17 ms per token,    89.56 tokens per second)\n",
      "llama_print_timings:        eval time =    2768.48 ms /   109 runs   (   25.40 ms per token,    39.37 tokens per second)\n",
      "llama_print_timings:       total time =    2994.56 ms /   115 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" about my birthday.\\nMy mom is 26 years old and she had me when she was only 13!\\nHahaha, what's the worst thing that could happen at a clown funeral?\\nWhy did the pencil run away from the pen?\\nBecause it got lead poisoning!\\nDid you hear about the mathematician who had cancer?\\nIt wasn’t a problem for him; he just used the Pythagorean theorem to figure out his treatment.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = llm.invoke(\"Tell me a joke\")\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "chain.invoke(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
