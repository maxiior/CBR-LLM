{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from models/llama-13b-hf_q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q8_0:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 36 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 36/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size = 13189.86 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 11572.03 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:  CUDA_Host KV buffer size =   160.00 MiB\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1440.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    72.03 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   816.01 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =   800.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 5\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n"
     ]
    }
   ],
   "source": [
    "n_gpu_layers = 1000  # Metal set to 1 is enough.\n",
    "n_gpu_layers = 36  # Metal set to 1 is enough.\n",
    "# n_gpu_layers = None  # Metal set to 1 is enough.\n",
    "n_batch = 2048  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"models/llama-13b-hf_q8_0.gguf\",\n",
    "    n_gpu_layers=n_gpu_layers,\n",
    "    n_batch=n_batch,\n",
    "    n_ctx=2048,\n",
    "    f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = \"Your task is to process and complete recipes that have missing data. You intelligently infer and fill in missing data based on complete examples. \\\n",
    "    A recipe has form of three columns, where at least one column has a value and the others can be masked. Columns:\\n\\nname: <NAME>\\ningredients: <INGREDIENTS>\\nsteps: <STEPS>\\n\\nContained \\\n",
    "    information:\\n\\n<NAME> is name of the recipe.\\n<INGREDIENTS> consists of ingredients used in processing actions described in steps, separated by commas.\\n<STEPS> are short sentences. \\\n",
    "    They begin with big letter and end with a dot. One step is one sentence. Steps explain what actions are needed to perform recipe in a correct manner.\\n\\n\\\n",
    "    Fill in the missing information in given recipe as accurately, logically, coherently, and truthfully as possible. It is necessary that the output is structured in the same way as shown in the examples above, where all columns contain values and the same structure.\\\n",
    "    This is a recipe to fill \\n\\nname: tilapia in mustard cream sauce\\ningredients: tilapia fillets, fresh thyme, fresh ground black pepper, salt, cooking spray, chicken broth, portabella mushroom, whipping cream, dijon mustard\\nsteps: <fill>\"\n",
    "\n",
    "req = \"Your task is to process and complete recipes that have missing data. You intelligently infer and fill in missing data based on complete examples. \\\n",
    "    A recipe has form of three columns, where at least one column has a value and the others can be masked. Columns: name: <NAME> ingredients: <INGREDIENTS> steps: <STEPS> Contained \\\n",
    "    information: <NAME> is name of the recipe. <INGREDIENTS> consists of ingredients used in processing actions described in steps, separated by commas. <STEPS> are short sentences. \\\n",
    "    They begin with big letter and end with a dot. One step is one sentence. Steps explain what actions are needed to perform recipe in a correct manner. \\\n",
    "    Fill in the missing information in given recipe as accurately, logically, coherently, and truthfully as possible. It is necessary that the output is structured in the same way as shown in the examples above, where all columns contain values and the same structure.\\\n",
    "    This is a recipe to fill - name: tilapia in mustard cream sauce ingredients: tilapia fillets, fresh thyme, fresh ground black pepper, salt, cooking spray, chicken broth, portabella mushroom, whipping cream, dijon mustard. Please provide the steps filling for this recipe.\"\n",
    "\n",
    "req = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name: Tilapia in mustard cream sauce ; ingredients: tilapia fillets, fresh thyme, fresh ground black pepper, salt, cooking spray, chicken broth, portabella mushroom, whipping cream, dijon mustard ; preparation:']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[f\"name: {e[2]} ; ingredients: {e[4]} ; preparation:\" for e in [df.iloc[[1,5,8]].values[-1]]]\n",
    "# [e for e in [df.iloc[[1,5,8]].values[-1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compose(gdf):\n",
    "    comp = [f\"name: {e[2]} ; ingredients: {e[4]} ; preparation:{e[3]}\" for e in gdf.values[:-1]]\n",
    "    # print(gdf.values[-1])\n",
    "    comp = comp + [f\"name: {e[2]} ; ingredients: {e[4]} ; preparation:\" for e in [gdf.values[-1]]]\n",
    "    return \"; \".join(comp)\n",
    "\n",
    "req = compose(df.iloc[[1,5,8]])\n",
    "req = \"The second planet in the solar system is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The second planet in the solar system is '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "req"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     195.59 ms\n",
      "llama_print_timings:      sample time =      45.72 ms /   256 runs   (    0.18 ms per token,  5599.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     433.51 ms /     9 tokens (   48.17 ms per token,    20.76 tokens per second)\n",
      "llama_print_timings:        eval time =   19503.41 ms /   255 runs   (   76.48 ms per token,    13.07 tokens per second)\n",
      "llama_print_timings:       total time =   20347.49 ms /   264 tokens\n"
     ]
    }
   ],
   "source": [
    "# res = llm.invoke(\"Simulate a rap battle between Stephen Colbert and John Oliver. Write only dialogue. Both of theme swear a lot and responses have to rhyme. They also fight on knives while rapping.\")\n",
    "res = llm.invoke(req)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9 billion years old and, by most accounts, still has plenty of life left in it.\n",
      "Mars’ surface looks like a cratered desert on Earth, but scientists believe there were once oceans and rivers there. There are signs of volcanic activity as well as canyons and valleys that suggest it was once habitable.\n",
      "The planet’s atmosphere was likely thin and made mostly out of carbon dioxide, with water vapor mixed in to help keep it warm. As its orbit shifted over time, however, the temperature dropped until all life was extinct – though some argue that this could have been prevented if only we had been able to preserve our atmosphere better!\n",
      "Nowadays Mars is cold enough to freeze carbon dioxide into dry ice; this substance will eventually turn into water vapor if exposed to enough sunlight or heat energy from volcanoes (which could help explain why some scientists think there may have been liquid water on Mars).\n",
      "What would happen if you went to Mars?\n",
      "If you went to Mars, you would be exposed to a lot of radiation. The atmosphere there is very thin, so you would be at risk for getting sunburns and other\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.9 billion years old and, by most accounts, still has plenty of life left in it.\\nMars’ surface looks like a cratered desert on Earth, but scientists believe there were once oceans and rivers there. There are signs of volcanic activity as well as canyons and valleys that suggest it was once habitable.\\nThe planet’s atmosphere was likely thin and made mostly out of carbon dioxide, with water vapor mixed in to help keep it warm. As its orbit shifted over time, however, the temperature dropped until all life was extinct – though some argue that this could have been prevented if only we had been able to preserve our atmosphere better!\\nNowadays Mars is cold enough to freeze carbon dioxide into dry ice; this substance will eventually turn into water vapor if exposed to enough sunlight or heat energy from volcanoes (which could help explain why some scientists think there may have been liquid water on Mars).\\nWhat would happen if you went to Mars?\\nIf you went to Mars, you would be exposed to a lot of radiation. The atmosphere there is very thin, so you would be at risk for getting sunburns and other'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.split(\"name:\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.9 billion years old and, by most accounts, still has plenty of life left in it.\\nMars’ surface looks like a cratered desert on Earth, but scientists believe there were once oceans and rivers there. There are signs of volcanic activity as well as canyons and valleys that suggest it was once habitable.\\nThe planet’s atmosphere was likely thin and made mostly out of carbon dioxide, with water vapor mixed in to help keep it warm. As its orbit shifted over time, however, the temperature dropped until all life was extinct – though some argue that this could have been prevented if only we had been able to preserve our atmosphere better!\\nNowadays Mars is cold enough to freeze carbon dioxide into dry ice; this substance will eventually turn into water vapor if exposed to enough sunlight or heat energy from volcanoes (which could help explain why some scientists think there may have been liquid water on Mars).\\nWhat would happen if you went to Mars?\\nIf you went to Mars, you would be exposed to a lot of radiation. The atmosphere there is very thin, so you would be at risk for getting sunburns and other'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response2 = \"\\n\\nStephen Colbert and John Oliver, two of the most popular late-night comedians, are facing off in a rap battle to end all rap battles. The stakes are high, with bragging rights on the line and the fate of the world hanging in the balance.\\n\\nHere's how the rap battle might go down:\\n\\nColbert:\\nYo, I'm the king of the late-night scene,\\nMy jokes are so sharp, they'll leave you serene.\\nI'm the master of satire, the one they all fear,\\nMy wit is so sharp, it'll leave you in tears.\\n\\nOliver:\\nHold up, Stephen, you think you're the best?\\nI've got news for you, you're just a mess.\\nMy jokes are like a sword, they'll pierce your soul,\\nI'm the one they call when they want to take control.\\n\\nColbert:\\nOh really, John? You think you can take me down?\\nI've got more Emmy's than you've got frowns.\\nMy show's the biggest,\"\n",
    "response = \".\\n\\nStephen Colbert:  Yo, I heard you tryna come for my crown, John Oliver!\\nI'm the king of late night, don't you forget!\\nMy jokes are fire, my wit is sharp,\\nI'm the one they call when they want to laugh hard!\\n\\nJohn Oliver:  Oh please, Stephen, you're just a clown,\\nYour humor's stale, your jokes are worn out.\\nI'm the one who brings the heat, the one who brings the pain,\\nMy comedy's like a sword, it's sharp and it's gained!\\n\\nStephen Colbert:  You may have some clever lines, John, but you can't touch my style,\\nMy fans love me, they wouldn't trade me for a while!\\nI've been doing this for years, I'm a pro,\\nYou're just a Brit who thinks he can flow!\\n\\nJohn Oliver:  Oh snap, you're going down, Stephen, you're in the zone,\\nMy comedy's not just clever, it's a work of art, it's shown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Stephen Colbert:  Yo, I heard you tryna come for my crown, John Oliver!\n",
      "I'm the king of late night, don't you forget!\n",
      "My jokes are fire, my wit is sharp,\n",
      "I'm the one they call when they want to laugh hard!\n",
      "\n",
      "John Oliver:  Oh please, Stephen, you're just a clown,\n",
      "Your humor's stale, your jokes are worn out.\n",
      "I'm the one who brings the heat, the one who brings the pain,\n",
      "My comedy's like a sword, it's sharp and it's gained!\n",
      "\n",
      "Stephen Colbert:  You may have some clever lines, John, but you can't touch my style,\n",
      "My fans love me, they wouldn't trade me for a while!\n",
      "I've been doing this for years, I'm a pro,\n",
      "You're just a Brit who thinks he can flow!\n",
      "\n",
      "John Oliver:  Oh snap, you're going down, Stephen, you're in the zone,\n",
      "My comedy's not just clever, it's a work of art, it's shown\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
