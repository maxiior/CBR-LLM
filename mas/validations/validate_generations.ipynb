{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex #xd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFY = \"_validation.csv\"\n",
    "# DFG = \"small_validation_pe_results_p.csv\"\n",
    "DFG = \"small_validation_cbr_00_results_p.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy = pd.read_csv(DFY)\n",
    "dfg = pd.read_csv(DFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_datestet(df1:pd.DataFrame, df2:pd.DataFrame):\n",
    "    return np.all(df1.target_recipe.values == df2.id.values)\n",
    "\n",
    "validate_datestet(dfg, dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2125, 2125)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dfy.steps.values\n",
    "pred = dfg.response.values\n",
    "\n",
    "len(y), len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "class Validation():\n",
    "    def __init__(self, experiment_configs) -> None:\n",
    "        self.validation_configs = experiment_configs.validation\n",
    "        self.prompt_configs = experiment_configs.prompt\n",
    "\n",
    "    def _cosine_similarity(self, original:str, generated:str) -> float:\n",
    "        tokenizer = GPT4AllEmbeddings()\n",
    "        X = tokenizer.embed_query(generated)\n",
    "        Y = tokenizer.embed_query(original)\n",
    "        return round(cosine_similarity([X], [Y]), 6)\n",
    "\n",
    "    def _bleu(self, original:str, generated:str, version:int=4) -> float:\n",
    "        weights = [0.25, 0.25, 0.25, 0.25]\n",
    "        if version == 3:\n",
    "            weights = [0.33, 0.33, 0.33]\n",
    "        elif version == 2:\n",
    "            weights = [0.5, 0.5]\n",
    "\n",
    "        return round(sentence_bleu([original.split()], generated.split(), weights=weights), 6)\n",
    "    \n",
    "    def _meteor(self, original:str, generated:str) -> float:\n",
    "        return round(meteor_score([original.split()], generated.split()), 6)\n",
    "\n",
    "    def validate(self, original, experiment_name):\n",
    "        generated = pd.read_csv(f'./logs/{experiment_name}_results.csv')['response'].to_lsit()\n",
    "        original_steps = original['steps'].to_list()\n",
    "        \n",
    "        x = 0\n",
    "\n",
    "        for i, j in zip(original_steps, generated):\n",
    "            if (\n",
    "                self._bleu(i, j, version=4) >= self.validation_configs.bleu_4_threshold and \n",
    "                self._bleu(i, j, version=3) >= self.validation_configs.bleu_3_threshold and \n",
    "                self._bleu(i, j, version=2) >= self.validation_configs.bleu_2_threshold and\n",
    "                self._meteor(i, j) >= self.validation_configs.meteor_threshold and\n",
    "                self._cosine_similarity(i, j) >= self.validation_configs.cosine_similarity_threshold\n",
    "            ):\n",
    "                x += 1\n",
    "        \n",
    "        return x, len(original_steps)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def score_bleu(pred:str, y:str, type=2): \n",
    "    if type == 4:\n",
    "        weights = [0.25, 0.25, 0.25, 0.25]\n",
    "    elif type == 3:\n",
    "        weights = [0.33, 0.33, 0.33]\n",
    "    elif type == 2:\n",
    "        weights = [0.5, 0.5]\n",
    "    res = []\n",
    "    for xe, ye in zip(pred, y):\n",
    "        res.append(sentence_bleu([list(xe)], list(ye), weights))\n",
    "    \n",
    "    return np.average(res)\n",
    "    \n",
    "def _embedding_cosine_similarity(x:str, y:str, tokenizer) -> float:\n",
    "    X = tokenizer.embed_query(x)\n",
    "    Y = tokenizer.embed_query(y)\n",
    "    return cosine_similarity([X], [Y])\n",
    "\n",
    "def embedding_cosine_similarity(x:str, y:str) -> float:\n",
    "    tokenizer = GPT4AllEmbeddings()\n",
    "    res = []\n",
    "    for xe, ye in zip(x, y):\n",
    "        res.append(_embedding_cosine_similarity(xe, ye, tokenizer))\n",
    "    return np.average(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bl2 = score_bleu(y, pred, 2)\n",
    "bl3 = score_bleu(y, pred, 3)\n",
    "bl4 = score_bleu(y, pred, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5288748034721343, 0.4460179789180792, 0.37680758056038033)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bl2, bl3, bl4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7598071288672484"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_cosine_similarity(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPE:\\nembedding_sim: 0.77055\\nbl2: 0.54114\\nbl3: 0.45684\\nbl4: 0.38604\\n\\nCBR_00:\\nembedding_sim: \\nbl2: \\nbl3: \\nbl4: \\n\\n\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PE:\n",
    "embedding_sim: 0.77055\n",
    "bl2: 0.54114\n",
    "bl3: 0.45684\n",
    "bl4: 0.38604\n",
    "\n",
    "CBR_00:\n",
    "embedding_sim: \n",
    "bl2: \n",
    "bl3: \n",
    "bl4: \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
