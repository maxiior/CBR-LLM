{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import LlamaCpp\n",
    "\n",
    "import abc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def send_request(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class LlamaCPP(Model):\n",
    "    def __init__(self, model_path=\"models/llama-13b-hf_q8_0.gguf\", n_gpu_layers=41, n_batch=1024, n_ctx=2048) -> None:\n",
    "        self.model_path = model_path\n",
    "        self.n_gpu_layers = n_gpu_layers\n",
    "        self.n_batch = n_batch\n",
    "        self.n_ctx = n_ctx\n",
    "\n",
    "        self.model = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_batch=n_batch,\n",
    "            n_ctx=n_ctx,\n",
    "            f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls\n",
    "            verbose=True,\n",
    "        )\n",
    "        super().__init__()\n",
    "\n",
    "    def send_request(self, X, break_word:str = \" name:\") -> str:\n",
    "        tok_len = self.model.get_num_tokens(X)\n",
    "        print(tok_len)\n",
    "        if tok_len > 1500:\n",
    "            raise Exception(f\"Request exceeds prefelable 1500 tokens. Has: {tok_len}\")\n",
    "        prev = \"\"\n",
    "        res = \"\"\n",
    "        for token in self.model.stream(X, echo=False):\n",
    "            res += token\n",
    "            if break_word == prev+token:\n",
    "                # print(res)#dev\n",
    "                res = res.replace(\" ; name:\", \"\")\n",
    "                res = res.replace(\";name:\", \"\")\n",
    "                res = res.replace(\" ;name:\", \"\")\n",
    "                res = res.replace(\"; name:\", \"\")\n",
    "                res = res.replace(\" name:\", \"\")\n",
    "                res = res.replace(\"name:\", \"\")\n",
    "                break\n",
    "            prev = token\n",
    "        \n",
    "        res = res.replace(\"  ;  \", \"\")\n",
    "        res = res.replace(\"  ; \", \"\")\n",
    "        res = res.replace(\"  ;\", \"\")\n",
    "        res = res.replace(\" ; \", \"\")\n",
    "        res = res.replace(\"; \", \"\")\n",
    "        res = res.replace(\" ;\", \"\")\n",
    "        res = res.replace(\";\", \"\")\n",
    "        return res\n",
    "\n",
    "    def get_info(self) -> str:\n",
    "        return {\n",
    "            \"model\":\"LLama-13b-hf-q8_0\",\n",
    "            \"model_path\":self.model_path,\n",
    "            \"n_gpu_layers\":self.n_gpu_layers,\n",
    "            \"n_batch\":self.n_batch,\n",
    "            \"n_ctx\":self.n_ctx,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test libs\n",
    "\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from transformers import LlamaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test request\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "def compose(gdf):\n",
    "    comp = [f\"name: {e[2]} ; ingredients: {e[4]} ; preparation:{e[3]}\" for e in gdf.values[:-1]]\n",
    "    # print(gdf.values[-1])\n",
    "    comp = comp + [f\"name: {e[2]} ; ingredients: {e[4]} ; preparation:\" for e in [gdf.values[-1]]]\n",
    "    return \" ; \".join(comp)\n",
    "\n",
    "# req = compose(df.iloc[[11, 15, 112, 122, 133, 144, 155, 8]])\n",
    "# req = \"The second planet in the solar system is \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LlamaCPP()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "req = compose(df.iloc[[random.randint(0, len(df)) for _ in range(4)]])\n",
    "print(req)\n",
    "res = model.send_request(req) # przykład send request\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "I_ITER = 100\n",
    "\n",
    "\n",
    "reqs = []\n",
    "ress = []\n",
    "\n",
    "start = time.time()\n",
    "for _ in range(I_ITER):\n",
    "    req = compose(df.iloc[[random.randint(0, len(df)) for _ in range(8)]])\n",
    "\n",
    "    res = model.send_request(req) # przykład send request\n",
    "\n",
    "    reqs.append(req)\n",
    "    ress.append(res)\n",
    "    print(\"----------------------------------------------------------------------------------------------------------------------------------------\")\n",
    "    print(req)\n",
    "    print(res)\n",
    "end = time.time()\n",
    "print((end - start)/I_ITER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ress"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
