{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n",
      "llama_model_loader: loaded meta data with 20 key-value pairs and 363 tensors from llama_models/llama-13b-hf_q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 5120\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 40\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 13824\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 40\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 40\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q8_0:  282 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 5120\n",
      "llm_load_print_meta: n_head           = 40\n",
      "llm_load_print_meta: n_head_kv        = 40\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 5120\n",
      "llm_load_print_meta: n_embd_v_gqa     = 5120\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 13824\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 13B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 13.02 B\n",
      "llm_load_print_meta: model size       = 12.88 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.28 MiB\n",
      "llm_load_tensors: offloading 40 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 41/41 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   166.02 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size = 13023.85 MiB\n",
      "....................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =  1600.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1600.00 MiB, K (f16):  800.00 MiB, V (f16):  800.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    32.02 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   408.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    20.00 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 3\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '5120', 'llama.feed_forward_length': '13824', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '40', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '40', 'llama.attention.head_count_kv': '40', 'tokenizer.ggml.model': 'llama', 'general.file_type': '7'}\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      27.26 ms /   156 runs   (    0.17 ms per token,  5722.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1613.38 ms /  1456 tokens (    1.11 ms per token,   902.45 tokens per second)\n",
      "llama_print_timings:        eval time =   10354.05 ms /   155 runs   (   66.80 ms per token,    14.97 tokens per second)\n",
      "llama_print_timings:       total time =   12250.01 ms /  1611 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =       3.32 ms /    22 runs   (    0.15 ms per token,  6624.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1429.38 ms /  1049 tokens (    1.36 ms per token,   733.88 tokens per second)\n",
      "llama_print_timings:        eval time =    1361.29 ms /    21 runs   (   64.82 ms per token,    15.43 tokens per second)\n",
      "llama_print_timings:       total time =    2828.77 ms /  1070 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      15.11 ms /    89 runs   (    0.17 ms per token,  5890.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1446.26 ms /  1250 tokens (    1.16 ms per token,   864.30 tokens per second)\n",
      "llama_print_timings:        eval time =    5815.48 ms /    88 runs   (   66.08 ms per token,    15.13 tokens per second)\n",
      "llama_print_timings:       total time =    7418.89 ms /  1338 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      31.94 ms /   201 runs   (    0.16 ms per token,  6293.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1345.85 ms /  1109 tokens (    1.21 ms per token,   824.02 tokens per second)\n",
      "llama_print_timings:        eval time =   13163.70 ms /   200 runs   (   65.82 ms per token,    15.19 tokens per second)\n",
      "llama_print_timings:       total time =   14865.93 ms /  1309 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      32.19 ms /   185 runs   (    0.17 ms per token,  5747.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1330.28 ms /  1128 tokens (    1.18 ms per token,   847.94 tokens per second)\n",
      "llama_print_timings:        eval time =   12109.76 ms /   184 runs   (   65.81 ms per token,    15.19 tokens per second)\n",
      "llama_print_timings:       total time =   13771.35 ms /  1312 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      12.08 ms /    68 runs   (    0.18 ms per token,  5627.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1458.88 ms /  1217 tokens (    1.20 ms per token,   834.20 tokens per second)\n",
      "llama_print_timings:        eval time =    4402.22 ms /    67 runs   (   65.70 ms per token,    15.22 tokens per second)\n",
      "llama_print_timings:       total time =    5983.77 ms /  1284 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      43.48 ms /   256 runs   (    0.17 ms per token,  5888.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1314.38 ms /  1052 tokens (    1.25 ms per token,   800.38 tokens per second)\n",
      "llama_print_timings:        eval time =   15167.24 ms /   255 runs   (   59.48 ms per token,    16.81 tokens per second)\n",
      "llama_print_timings:       total time =   16940.18 ms /  1307 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      27.77 ms /   164 runs   (    0.17 ms per token,  5906.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1302.56 ms /  1037 tokens (    1.26 ms per token,   796.13 tokens per second)\n",
      "llama_print_timings:        eval time =   10847.84 ms /   163 runs   (   66.55 ms per token,    15.03 tokens per second)\n",
      "llama_print_timings:       total time =   12450.90 ms /  1200 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      45.78 ms /   256 runs   (    0.18 ms per token,  5591.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1384.42 ms /  1148 tokens (    1.21 ms per token,   829.23 tokens per second)\n",
      "llama_print_timings:        eval time =   17207.20 ms /   255 runs   (   67.48 ms per token,    14.82 tokens per second)\n",
      "llama_print_timings:       total time =   19068.89 ms /  1403 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      19.42 ms /   118 runs   (    0.16 ms per token,  6075.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1386.24 ms /  1125 tokens (    1.23 ms per token,   811.55 tokens per second)\n",
      "llama_print_timings:        eval time =    7901.69 ms /   117 runs   (   67.54 ms per token,    14.81 tokens per second)\n",
      "llama_print_timings:       total time =    9498.71 ms /  1242 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      40.70 ms /   234 runs   (    0.17 ms per token,  5749.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1366.96 ms /  1093 tokens (    1.25 ms per token,   799.58 tokens per second)\n",
      "llama_print_timings:        eval time =   18162.84 ms /   233 runs   (   77.95 ms per token,    12.83 tokens per second)\n",
      "llama_print_timings:       total time =   19965.57 ms /  1326 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      31.60 ms /   178 runs   (    0.18 ms per token,  5633.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1552.50 ms /  1284 tokens (    1.21 ms per token,   827.05 tokens per second)\n",
      "llama_print_timings:        eval time =   13599.49 ms /   177 runs   (   76.83 ms per token,    13.02 tokens per second)\n",
      "llama_print_timings:       total time =   15479.49 ms /  1461 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      17.57 ms /   107 runs   (    0.16 ms per token,  6089.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1578.31 ms /  1411 tokens (    1.12 ms per token,   894.00 tokens per second)\n",
      "llama_print_timings:        eval time =    7239.56 ms /   106 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =    9010.12 ms /  1517 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      42.36 ms /   250 runs   (    0.17 ms per token,  5901.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =     986.49 ms /   991 tokens (    1.00 ms per token,  1004.57 tokens per second)\n",
      "llama_print_timings:        eval time =   16622.21 ms /   249 runs   (   66.76 ms per token,    14.98 tokens per second)\n",
      "llama_print_timings:       total time =   18063.98 ms /  1240 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      21.60 ms /   121 runs   (    0.18 ms per token,  5601.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1452.64 ms /  1240 tokens (    1.17 ms per token,   853.62 tokens per second)\n",
      "llama_print_timings:        eval time =    8025.36 ms /   120 runs   (   66.88 ms per token,    14.95 tokens per second)\n",
      "llama_print_timings:       total time =    9702.08 ms /  1360 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      19.44 ms /   110 runs   (    0.18 ms per token,  5659.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1386.12 ms /  1153 tokens (    1.20 ms per token,   831.82 tokens per second)\n",
      "llama_print_timings:        eval time =    7307.54 ms /   109 runs   (   67.04 ms per token,    14.92 tokens per second)\n",
      "llama_print_timings:       total time =    8895.55 ms /  1262 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      44.17 ms /   256 runs   (    0.17 ms per token,  5796.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1304.10 ms /  1164 tokens (    1.12 ms per token,   892.57 tokens per second)\n",
      "llama_print_timings:        eval time =   16111.92 ms /   255 runs   (   63.18 ms per token,    15.83 tokens per second)\n",
      "llama_print_timings:       total time =   17893.06 ms /  1419 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      45.31 ms /   256 runs   (    0.18 ms per token,  5650.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1353.15 ms /  1076 tokens (    1.26 ms per token,   795.18 tokens per second)\n",
      "llama_print_timings:        eval time =   17154.96 ms /   255 runs   (   67.27 ms per token,    14.86 tokens per second)\n",
      "llama_print_timings:       total time =   18977.45 ms /  1331 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =       9.00 ms /    56 runs   (    0.16 ms per token,  6225.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     996.59 ms /  1012 tokens (    0.98 ms per token,  1015.47 tokens per second)\n",
      "llama_print_timings:        eval time =    3735.81 ms /    55 runs   (   67.92 ms per token,    14.72 tokens per second)\n",
      "llama_print_timings:       total time =    4830.66 ms /  1067 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      44.15 ms /   256 runs   (    0.17 ms per token,  5798.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1493.43 ms /  1299 tokens (    1.15 ms per token,   869.81 tokens per second)\n",
      "llama_print_timings:        eval time =   17345.05 ms /   255 runs   (   68.02 ms per token,    14.70 tokens per second)\n",
      "llama_print_timings:       total time =   19312.55 ms /  1554 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      31.96 ms /   200 runs   (    0.16 ms per token,  6257.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1467.65 ms /  1268 tokens (    1.16 ms per token,   863.97 tokens per second)\n",
      "llama_print_timings:        eval time =   13591.81 ms /   199 runs   (   68.30 ms per token,    14.64 tokens per second)\n",
      "llama_print_timings:       total time =   15422.22 ms /  1467 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      25.08 ms /   144 runs   (    0.17 ms per token,  5742.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1442.10 ms /  1165 tokens (    1.24 ms per token,   807.85 tokens per second)\n",
      "llama_print_timings:        eval time =   10102.81 ms /   143 runs   (   70.65 ms per token,    14.15 tokens per second)\n",
      "llama_print_timings:       total time =   11806.72 ms /  1308 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      11.96 ms /    69 runs   (    0.17 ms per token,  5768.27 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1417.66 ms /  1198 tokens (    1.18 ms per token,   845.05 tokens per second)\n",
      "llama_print_timings:        eval time =    4549.98 ms /    68 runs   (   66.91 ms per token,    14.95 tokens per second)\n",
      "llama_print_timings:       total time =    6088.31 ms /  1266 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      28.48 ms /   170 runs   (    0.17 ms per token,  5969.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1319.64 ms /  1064 tokens (    1.24 ms per token,   806.28 tokens per second)\n",
      "llama_print_timings:        eval time =   11392.62 ms /   169 runs   (   67.41 ms per token,    14.83 tokens per second)\n",
      "llama_print_timings:       total time =   13018.79 ms /  1233 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      35.90 ms /   197 runs   (    0.18 ms per token,  5487.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11475.20 ms /  1188 tokens (    9.66 ms per token,   103.53 tokens per second)\n",
      "llama_print_timings:        eval time =  108036.59 ms /   196 runs   (  551.21 ms per token,     1.81 tokens per second)\n",
      "llama_print_timings:       total time =  119896.82 ms /  1384 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      25.80 ms /   148 runs   (    0.17 ms per token,  5737.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11398.15 ms /  1417 tokens (    8.04 ms per token,   124.32 tokens per second)\n",
      "llama_print_timings:        eval time =  102960.69 ms /   147 runs   (  700.41 ms per token,     1.43 tokens per second)\n",
      "llama_print_timings:       total time =  114650.59 ms /  1564 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      43.94 ms /   256 runs   (    0.17 ms per token,  5826.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9643.55 ms /  1275 tokens (    7.56 ms per token,   132.21 tokens per second)\n",
      "llama_print_timings:        eval time =  158555.82 ms /   255 runs   (  621.79 ms per token,     1.61 tokens per second)\n",
      "llama_print_timings:       total time =  168699.13 ms /  1530 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      45.64 ms /   256 runs   (    0.18 ms per token,  5608.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14075.30 ms /  1048 tokens (   13.43 ms per token,    74.46 tokens per second)\n",
      "llama_print_timings:        eval time =  176489.60 ms /   255 runs   (  692.12 ms per token,     1.44 tokens per second)\n",
      "llama_print_timings:       total time =  191070.70 ms /  1303 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      17.47 ms /   103 runs   (    0.17 ms per token,  5895.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1013.26 ms /   990 tokens (    1.02 ms per token,   977.05 tokens per second)\n",
      "llama_print_timings:        eval time =    7732.04 ms /   102 runs   (   75.80 ms per token,    13.19 tokens per second)\n",
      "llama_print_timings:       total time =    8925.29 ms /  1092 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      16.88 ms /   103 runs   (    0.16 ms per token,  6101.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1670.25 ms /  1310 tokens (    1.28 ms per token,   784.31 tokens per second)\n",
      "llama_print_timings:        eval time =    7042.24 ms /   102 runs   (   69.04 ms per token,    14.48 tokens per second)\n",
      "llama_print_timings:       total time =    8894.26 ms /  1412 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      42.58 ms /   256 runs   (    0.17 ms per token,  6011.79 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1078.14 ms /  1021 tokens (    1.06 ms per token,   947.00 tokens per second)\n",
      "llama_print_timings:        eval time =   15822.01 ms /   255 runs   (   62.05 ms per token,    16.12 tokens per second)\n",
      "llama_print_timings:       total time =   17352.66 ms /  1276 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =       5.39 ms /    33 runs   (    0.16 ms per token,  6127.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1280.18 ms /  1095 tokens (    1.17 ms per token,   855.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1945.30 ms /    32 runs   (   60.79 ms per token,    16.45 tokens per second)\n",
      "llama_print_timings:       total time =    3280.58 ms /  1127 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      41.87 ms /   256 runs   (    0.16 ms per token,  6113.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1450.07 ms /  1392 tokens (    1.04 ms per token,   959.95 tokens per second)\n",
      "llama_print_timings:        eval time =   16273.50 ms /   255 runs   (   63.82 ms per token,    15.67 tokens per second)\n",
      "llama_print_timings:       total time =   18184.89 ms /  1647 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      19.88 ms /   115 runs   (    0.17 ms per token,  5784.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1459.77 ms /  1277 tokens (    1.14 ms per token,   874.80 tokens per second)\n",
      "llama_print_timings:        eval time =    7420.54 ms /   114 runs   (   65.09 ms per token,    15.36 tokens per second)\n",
      "llama_print_timings:       total time =    9081.56 ms /  1391 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      20.96 ms /   137 runs   (    0.15 ms per token,  6537.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1354.50 ms /  1250 tokens (    1.08 ms per token,   922.85 tokens per second)\n",
      "llama_print_timings:        eval time =    8296.90 ms /   136 runs   (   61.01 ms per token,    16.39 tokens per second)\n",
      "llama_print_timings:       total time =    9885.04 ms /  1386 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      34.26 ms /   202 runs   (    0.17 ms per token,  5895.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1400.68 ms /  1155 tokens (    1.21 ms per token,   824.60 tokens per second)\n",
      "llama_print_timings:        eval time =   15466.02 ms /   201 runs   (   76.95 ms per token,    13.00 tokens per second)\n",
      "llama_print_timings:       total time =   17238.22 ms /  1356 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      34.59 ms /   209 runs   (    0.17 ms per token,  6042.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1471.81 ms /  1204 tokens (    1.22 ms per token,   818.04 tokens per second)\n",
      "llama_print_timings:        eval time =   18867.48 ms /   208 runs   (   90.71 ms per token,    11.02 tokens per second)\n",
      "llama_print_timings:       total time =   20727.69 ms /  1412 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      25.44 ms /   153 runs   (    0.17 ms per token,  6014.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1506.89 ms /  1232 tokens (    1.22 ms per token,   817.58 tokens per second)\n",
      "llama_print_timings:        eval time =   11385.39 ms /   152 runs   (   74.90 ms per token,    13.35 tokens per second)\n",
      "llama_print_timings:       total time =   13167.43 ms /  1384 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      22.17 ms /   133 runs   (    0.17 ms per token,  5999.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1531.31 ms /  1051 tokens (    1.46 ms per token,   686.34 tokens per second)\n",
      "llama_print_timings:        eval time =   12202.36 ms /   132 runs   (   92.44 ms per token,    10.82 tokens per second)\n",
      "llama_print_timings:       total time =   13977.01 ms /  1183 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      42.43 ms /   256 runs   (    0.17 ms per token,  6033.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1598.28 ms /  1294 tokens (    1.24 ms per token,   809.62 tokens per second)\n",
      "llama_print_timings:        eval time =   19217.33 ms /   255 runs   (   75.36 ms per token,    13.27 tokens per second)\n",
      "llama_print_timings:       total time =   21291.25 ms /  1549 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      25.90 ms /   154 runs   (    0.17 ms per token,  5945.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1642.48 ms /  1210 tokens (    1.36 ms per token,   736.69 tokens per second)\n",
      "llama_print_timings:        eval time =   11319.38 ms /   153 runs   (   73.98 ms per token,    13.52 tokens per second)\n",
      "llama_print_timings:       total time =   13237.40 ms /  1363 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      35.06 ms /   210 runs   (    0.17 ms per token,  5989.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1445.10 ms /  1203 tokens (    1.20 ms per token,   832.47 tokens per second)\n",
      "llama_print_timings:        eval time =   14566.70 ms /   209 runs   (   69.70 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =   16393.17 ms /  1412 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      29.80 ms /   174 runs   (    0.17 ms per token,  5839.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1371.00 ms /  1105 tokens (    1.24 ms per token,   805.98 tokens per second)\n",
      "llama_print_timings:        eval time =   12059.79 ms /   173 runs   (   69.71 ms per token,    14.35 tokens per second)\n",
      "llama_print_timings:       total time =   13749.24 ms /  1278 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1545.78 ms\n",
      "llama_print_timings:      sample time =      42.81 ms /   256 runs   (    0.17 ms per token,  5979.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =     959.82 ms /   955 tokens (    1.01 ms per token,   994.98 tokens per second)\n",
      "llama_print_timings:        eval time =   17523.41 ms /   255 runs   (   68.72 ms per token,    14.55 tokens per second)\n",
      "llama_print_timings:       total time =   18946.73 ms /  1210 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Request exceeds prefelable 1500 tokens. Has: 1508",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m llama \u001b[38;5;241m=\u001b[39m LlamaCPP(model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama_models/llama-13b-hf_q8_0.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# run(model=llama, dataset_name=\"test_input.csv\", experiment_name='__________________test')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# run(model=llama, dataset_name=\"small_validation_pe_input.csv\", experiment_name='small_validation_pe_2')\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllama\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msmall_validation_cbr_00_input.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmall_validation_cbr_00\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# run(model=llama, dataset_name=\"cbr_augmentation_2_input.csv\", experiment_name='cbr_augmentation_2')\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# run(model=llama, dataset_name=\"cbr_augmentation_2_input_continue.csv\", experiment_name='cbr_augmentation_2_input_continue')\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# run(model=llama, dataset_name=\"cbr_augmentation_3_input.csv\", experiment_name='cbr_augmentation_3')\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/LLM-IN-CBR/src/experiment.py:45\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(model, dataset_name, experiment_name)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_recipe\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 45\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     responses\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mlen\u001b[39m(responses)] \u001b[38;5;241m=\u001b[39m [response, \u001b[38;5;28mid\u001b[39m]\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/projects/LLM-IN-CBR/src/models/llama.py:27\u001b[0m, in \u001b[0;36mLlamaCPP.send_request\u001b[0;34m(self, X, break_word)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# print(tok_len)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tok_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1500\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequest exceeds prefelable 1500 tokens. Has: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtok_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m prev \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mException\u001b[0m: Request exceeds prefelable 1500 tokens. Has: 1508"
     ]
    }
   ],
   "source": [
    "from experiment import run \n",
    "from models.llama import LlamaCPP \n",
    "import langchain\n",
    "langchain.debug = False #dx\n",
    "\n",
    "llama = LlamaCPP(model_path=\"llama_models/llama-13b-hf_q8_0.gguf\")\n",
    "# run(model=llama, dataset_name=\"test_input.csv\", experiment_name='__________________test')\n",
    "\n",
    "# run(model=llama, dataset_name=\"small_validation_pe_input.csv\", experiment_name='small_validation_pe_2')\n",
    "run(model=llama, dataset_name=\"small_validation_cbr_00_input_continue.csv\", experiment_name='small_validation_cbr_00_continue')\n",
    "\n",
    "# run(model=llama, dataset_name=\"cbr_augmentation_2_input.csv\", experiment_name='cbr_augmentation_2')\n",
    "# run(model=llama, dataset_name=\"cbr_augmentation_2_input_continue.csv\", experiment_name='cbr_augmentation_2_input_continue')\n",
    "# run(model=llama, dataset_name=\"cbr_augmentation_3_input.csv\", experiment_name='cbr_augmentation_3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
