{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "import numpy as np\n",
    "import regex #dev\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_bleu(pred:str, y:str, type=2): \n",
    "    if type == 4:\n",
    "        weights = [0.25, 0.25, 0.25, 0.25]\n",
    "    elif type == 3:\n",
    "        weights = [0.33, 0.33, 0.33]\n",
    "    elif type == 2:\n",
    "        weights = [0.5, 0.5]\n",
    "    res = []\n",
    "    for xe, ye in zip(pred, y):\n",
    "        res.append(sentence_bleu([list(xe)], list(ye), weights))\n",
    "    \n",
    "    return np.average(res)\n",
    "    \n",
    "def _embedding_cosine_similarity(x:str, y:str, embedding_function) -> float:\n",
    "    X = embedding_function.embed_query(x)\n",
    "    Y = embedding_function.embed_query(y)\n",
    "    return cosine_similarity([X], [Y])\n",
    "\n",
    "def embedding_cosine_similarity(x:str, y:str, embedding_function) -> float:\n",
    "    res = []\n",
    "    for xe, ye in zip(x, y):\n",
    "        res.append(_embedding_cosine_similarity(xe, ye, embedding_function))\n",
    "    return np.average(res)\n",
    "\n",
    "def score_meteor(pred:str, y:str) -> float:\n",
    "    res = []\n",
    "    for xe, ye in zip(pred, y):\n",
    "        res.append(meteor_score([xe.split()], ye.split()))\n",
    "    \n",
    "    return np.average(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import random\n",
    "\n",
    "\n",
    "def calculate_threasholds(comp_samples:np.array, coverage_procentage:float=0.1, embedding_function=GPT4AllEmbeddings(), info:str=\"\"):\n",
    "    \n",
    "    samples_c = len(comp_samples)\n",
    "\n",
    "    c_range = ceil(samples_c*coverage_procentage)\n",
    "    print(f\"Making {c_range} comparisons...\")#dev\n",
    "    comp_ids_1 = [random.randrange(0, samples_c-1) for _ in range(c_range)]\n",
    "    comp_ids_2 = [random.randrange(0, samples_c-1) for _ in range(c_range)]\n",
    "\n",
    "    bleu_2 = score_bleu(comp_samples[comp_ids_1], comp_samples[comp_ids_2], 2)\n",
    "    print(\"bleu_2\")\n",
    "    bleu_3 = score_bleu(comp_samples[comp_ids_1], comp_samples[comp_ids_2], 3)\n",
    "    print(\"bleu_3\")\n",
    "    bleu_4 = score_bleu(comp_samples[comp_ids_1], comp_samples[comp_ids_2], 4)\n",
    "    print(\"bleu_4\")\n",
    "    meteor = score_meteor(comp_samples[comp_ids_1], comp_samples[comp_ids_2])\n",
    "    print(\"meteor\")\n",
    "    cosine_sim = embedding_cosine_similarity(comp_samples[comp_ids_1], comp_samples[comp_ids_2], embedding_function)\n",
    "    print(\"cosine_sim\")\n",
    "\n",
    "    adder_metrics_threasholds = {\"bleu_2\":bleu_2, \"bleu_3\":bleu_3, \"bleu_4\":bleu_4, \"meteor\":meteor, \"cosine_sim\":cosine_sim, \"info\":info}\n",
    "    \n",
    "    return adder_metrics_threasholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = \"_cbr_database\"\n",
    "\n",
    "df = pd.read_csv(f\"{DATABASE_FILE}.csv\")\n",
    "samples = df.steps.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_threasholds(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000\n",
    "metrics_threasholds = {'bleu_2': 0.4854855502611209,\n",
    " 'bleu_3': 0.3674074412576021,\n",
    " 'bleu_4': 0.2722650963284384,\n",
    " 'meteor': 0.1246965969044918,\n",
    " 'cosine_sim': 0.42583562424830085,\n",
    " 'info': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _validate_example(text:str, db_examples:pd.DataFrame, adder_metrics_threasholds, ic:int, ez:float, embedding_function) -> bool:\n",
    "    input_texts = [text for _ in range(ic)]\n",
    "\n",
    "    comp_samples = db_examples #np.array(self.get_texts())\n",
    "    samples_c = len(comp_samples)\n",
    "\n",
    "    c_range = ic\n",
    "    print(f\"Making {c_range} comparisons...\")#dev\n",
    "    comp_ids_1 = [random.randrange(0, samples_c-1) for _ in range(c_range)]\n",
    "\n",
    "    bleu_2 = score_bleu(comp_samples[comp_ids_1], input_texts, 2)\n",
    "    print(f\"bleu_2: {bleu_2} vs {adder_metrics_threasholds['bleu_2']}\")\n",
    "    if bleu_2 < adder_metrics_threasholds[\"bleu_2\"]*ez: return False\n",
    "\n",
    "    bleu_3 = score_bleu(comp_samples[comp_ids_1], input_texts, 3)\n",
    "    print(f\"bleu_3: {bleu_3} vs {adder_metrics_threasholds['bleu_3']}\")\n",
    "    if bleu_3 < adder_metrics_threasholds[\"bleu_3\"]*ez: return False\n",
    "\n",
    "    bleu_4 = score_bleu(comp_samples[comp_ids_1], input_texts, 4)\n",
    "    print(f\"bleu_4: {bleu_4} vs {adder_metrics_threasholds['bleu_4']}\")\n",
    "    if bleu_4 < adder_metrics_threasholds[\"bleu_4\"]*ez: return False\n",
    "\n",
    "    meteor = score_meteor(comp_samples[comp_ids_1], input_texts)\n",
    "    print(f\"meteor: {meteor} vs {adder_metrics_threasholds['meteor']}\")\n",
    "    if meteor < adder_metrics_threasholds[\"meteor\"]*ez: return False\n",
    "\n",
    "    cosine_sim = embedding_cosine_similarity(comp_samples[comp_ids_1], input_texts, embedding_function)\n",
    "    print(f\"cosine_sim: {cosine_sim} vs {adder_metrics_threasholds['cosine_sim']}\")\n",
    "    if cosine_sim < adder_metrics_threasholds[\"cosine_sim\"]*ez: return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "\n",
    "def validate_examples(texts, db_examples:pd.DataFrame, adder_metrics_threasholds, ic:int=100, ez=0.9, embedding_function=GPT4AllEmbeddings()):\n",
    "    \"\"\"Adds new examples to vector database\n",
    "\n",
    "    :param list[str] texts: _description_\n",
    "    :param list[dict] metadatas: _description_, defaults to None\n",
    "    \"\"\"        \n",
    "    res_mask = []\n",
    "\n",
    "    for txt in texts:\n",
    "        if _validate_example(txt, db_examples, adder_metrics_threasholds=adder_metrics_threasholds, ic=ic, ez=ez, embedding_function=embedding_function): \n",
    "            res_mask.append(True)\n",
    "        else:\n",
    "            res_mask.append(False)\n",
    "\n",
    "        # res_texts.append(txt)\n",
    "        # res_metadatas.append(md)\n",
    "\n",
    "    return res_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making 100 comparisons...\n",
      "bleu_2: 0.4947357268351817 vs 0.4854855502611209\n",
      "bleu_3: 0.36877896544488215 vs 0.3674074412576021\n",
      "bleu_4: 0.2698066636457497 vs 0.2722650963284384\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.5244537770719556 vs 0.4854855502611209\n",
      "bleu_3: 0.391233542248728 vs 0.3674074412576021\n",
      "bleu_4: 0.28776114381585743 vs 0.2722650963284384\n",
      "meteor: 0.1307768808288201 vs 0.1246965969044918\n",
      "cosine_sim: 0.3836346913821888 vs 0.42583562424830085\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.4874665782458089 vs 0.4854855502611209\n",
      "bleu_3: 0.3577174488140516 vs 0.3674074412576021\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.4740272312954781 vs 0.4854855502611209\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.5362016617416142 vs 0.4854855502611209\n",
      "bleu_3: 0.4076436863333317 vs 0.3674074412576021\n",
      "bleu_4: 0.30272347675522915 vs 0.2722650963284384\n",
      "meteor: 0.13949398026856838 vs 0.1246965969044918\n",
      "cosine_sim: 0.4104467330681848 vs 0.42583562424830085\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.5216923256544779 vs 0.4854855502611209\n",
      "bleu_3: 0.40064495572750447 vs 0.3674074412576021\n",
      "bleu_4: 0.29987972235728527 vs 0.2722650963284384\n",
      "meteor: 0.11929708261383094 vs 0.1246965969044918\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.5119927045978888 vs 0.4854855502611209\n",
      "bleu_3: 0.3809277204221707 vs 0.3674074412576021\n",
      "bleu_4: 0.2795171661796674 vs 0.2722650963284384\n",
      "meteor: 0.12597219369336432 vs 0.1246965969044918\n",
      "cosine_sim: 0.43836365882816447 vs 0.42583562424830085\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.5446566045293464 vs 0.4854855502611209\n",
      "bleu_3: 0.41683049521941823 vs 0.3674074412576021\n",
      "bleu_4: 0.3092989404816431 vs 0.2722650963284384\n",
      "meteor: 0.12996634493463408 vs 0.1246965969044918\n",
      "cosine_sim: 0.42647586254334935 vs 0.42583562424830085\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.36843727975928386 vs 0.4854855502611209\n",
      "Making 100 comparisons...\n",
      "bleu_2: 0.5167337326859925 vs 0.4854855502611209\n",
      "bleu_3: 0.39556040312477464 vs 0.3674074412576021\n",
      "bleu_4: 0.29449689182150074 vs 0.2722650963284384\n",
      "meteor: 0.137185541157786 vs 0.1246965969044918\n",
      "cosine_sim: 0.39243380272031586 vs 0.42583562424830085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[False, False, False, False, False, False, True, True, False, False]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_examples(samples[:10], samples, metrics_threasholds, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
