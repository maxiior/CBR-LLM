{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n",
    "import numpy as np\n",
    "import regex #dev\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from langchain_community.embeddings import GPT4AllEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_bleu(pred:str, y:str, type=2): \n",
    "    if type == 4:\n",
    "        weights = [0.25, 0.25, 0.25, 0.25]\n",
    "    elif type == 3:\n",
    "        weights = [0.33, 0.33, 0.33]\n",
    "    elif type == 2:\n",
    "        weights = [0.5, 0.5]\n",
    "    res = []\n",
    "    for xe, ye in zip(pred, y):\n",
    "        res.append(sentence_bleu([list(xe)], list(ye), weights))\n",
    "    \n",
    "    return np.average(res)\n",
    "    \n",
    "def _embedding_cosine_similarity(x:str, y:str, embedding_function) -> float:\n",
    "    X = embedding_function.embed_query(x)\n",
    "    Y = embedding_function.embed_query(y)\n",
    "    return cosine_similarity([X], [Y])\n",
    "\n",
    "def embedding_cosine_similarity(x:str, y:str, embedding_function) -> float:\n",
    "    res = []\n",
    "    for xe, ye in zip(x, y):\n",
    "        res.append(_embedding_cosine_similarity(xe, ye, embedding_function))\n",
    "    return np.average(res)\n",
    "\n",
    "def score_meteor(pred:str, y:str) -> float:\n",
    "    res = []\n",
    "    for xe, ye in zip(pred, y):\n",
    "        res.append(meteor_score([xe.split()], ye.split()))\n",
    "    \n",
    "    return np.average(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "import random\n",
    "\n",
    "\n",
    "def calculate_threasholds(comp_samples:np.array, coverage_procentage:float=0.1, embedding_function=GPT4AllEmbeddings(), info:str=\"\"):\n",
    "    \n",
    "    samples_c = len(comp_samples)\n",
    "\n",
    "    c_range = ceil(samples_c*coverage_procentage)\n",
    "    print(f\"Making {c_range} comparisons...\")#dev\n",
    "    comp_ids_1 = [random.randrange(0, samples_c-1) for _ in range(c_range)]\n",
    "    comp_ids_2 = [random.randrange(0, samples_c-1) for _ in range(c_range)]\n",
    "\n",
    "    bleu_2 = score_bleu(comp_samples[comp_ids_1], comp_samples[comp_ids_2], 2)\n",
    "    print(\"bleu_2\")\n",
    "    bleu_3 = score_bleu(comp_samples[comp_ids_1], comp_samples[comp_ids_2], 3)\n",
    "    print(\"bleu_3\")\n",
    "    bleu_4 = score_bleu(comp_samples[comp_ids_1], comp_samples[comp_ids_2], 4)\n",
    "    print(\"bleu_4\")\n",
    "    meteor = score_meteor(comp_samples[comp_ids_1], comp_samples[comp_ids_2])\n",
    "    print(\"meteor\")\n",
    "    cosine_sim = embedding_cosine_similarity(comp_samples[comp_ids_1], comp_samples[comp_ids_2], embedding_function)\n",
    "    print(\"cosine_sim\")\n",
    "\n",
    "    adder_metrics_threasholds = {\"bleu_2\":bleu_2, \"bleu_3\":bleu_3, \"bleu_4\":bleu_4, \"meteor\":meteor, \"cosine_sim\":cosine_sim, \"info\":info}\n",
    "    \n",
    "    return adder_metrics_threasholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATABASE_FILE = \"_cbr_database\"\n",
    "\n",
    "df = pd.read_csv(f\"{DATABASE_FILE}.csv\")\n",
    "samples = df.steps.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_threasholds(samples, coverage_procentage=3, info=\"3 coverage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000\n",
    "metrics_threasholds = {'bleu_2': 0.4854855502611209,\n",
    " 'bleu_3': 0.3674074412576021,\n",
    " 'bleu_4': 0.2722650963284384,\n",
    " 'meteor': 0.1246965969044918,\n",
    " 'cosine_sim': 0.42583562424830085,\n",
    " 'info': ''}\n",
    "\n",
    "# 30 k\n",
    "metrics_threasholds = {'bleu_2': 0.48610981609361903,\n",
    " 'bleu_3': 0.36762298708981883,\n",
    " 'bleu_4': 0.2725605103366232,\n",
    " 'meteor': 0.12422512973849278,\n",
    " 'cosine_sim': 0.4289302823367131,\n",
    " 'info': '3 coverage'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _validate_example(text:str, db_examples:pd.DataFrame, adder_metrics_threasholds, ic:int, ez:float, embedding_function) -> bool:\n",
    "    input_texts = [text for _ in range(ic)]\n",
    "\n",
    "    comp_samples = db_examples #np.array(self.get_texts())\n",
    "    samples_c = len(comp_samples)\n",
    "\n",
    "    c_range = ic\n",
    "    # print(f\"Making {c_range} comparisons...\")#dev\n",
    "    comp_ids_1 = [random.randrange(0, samples_c-1) for _ in range(c_range)]\n",
    "\n",
    "    bleu_2 = score_bleu(comp_samples[comp_ids_1], input_texts, 2)\n",
    "    # print(f\"bleu_2: {bleu_2} vs {adder_metrics_threasholds['bleu_2']}\")\n",
    "    if bleu_2 < adder_metrics_threasholds[\"bleu_2\"]*ez: return False\n",
    "\n",
    "    bleu_3 = score_bleu(comp_samples[comp_ids_1], input_texts, 3)\n",
    "    # print(f\"bleu_3: {bleu_3} vs {adder_metrics_threasholds['bleu_3']}\")\n",
    "    if bleu_3 < adder_metrics_threasholds[\"bleu_3\"]*ez: return False\n",
    "\n",
    "    bleu_4 = score_bleu(comp_samples[comp_ids_1], input_texts, 4)\n",
    "    # print(f\"bleu_4: {bleu_4} vs {adder_metrics_threasholds['bleu_4']}\")\n",
    "    if bleu_4 < adder_metrics_threasholds[\"bleu_4\"]*ez: return False\n",
    "\n",
    "    meteor = score_meteor(comp_samples[comp_ids_1], input_texts)\n",
    "    # print(f\"meteor: {meteor} vs {adder_metrics_threasholds['meteor']}\")\n",
    "    if meteor < adder_metrics_threasholds[\"meteor\"]*ez: return False\n",
    "\n",
    "    cosine_sim = embedding_cosine_similarity(comp_samples[comp_ids_1], input_texts, embedding_function)\n",
    "    # print(f\"cosine_sim: {cosine_sim} vs {adder_metrics_threasholds['cosine_sim']}\")\n",
    "    if cosine_sim < adder_metrics_threasholds[\"cosine_sim\"]*ez: return False\n",
    "\n",
    "    return True\n",
    "    \n",
    "\n",
    "def validate_examples(texts, db_examples:pd.DataFrame, adder_metrics_threasholds, ic:int=100, ez=0.9, embedding_function=GPT4AllEmbeddings()):\n",
    "    \"\"\"Adds new examples to vector database\n",
    "\n",
    "    :param list[str] texts: _description_\n",
    "    :param list[dict] metadatas: _description_, defaults to None\n",
    "    \"\"\"        \n",
    "    res_mask = []\n",
    "\n",
    "    for txt in texts:\n",
    "        if _validate_example(txt, db_examples, adder_metrics_threasholds=adder_metrics_threasholds, ic=ic, ez=ez, embedding_function=embedding_function): \n",
    "            res_mask.append(True)\n",
    "        else:\n",
    "            if _validate_example(txt, db_examples, adder_metrics_threasholds=adder_metrics_threasholds, ic=ic, ez=ez, embedding_function=embedding_function): \n",
    "                res_mask.append(True)\n",
    "            else:\n",
    "                if _validate_example(txt, db_examples, adder_metrics_threasholds=adder_metrics_threasholds, ic=ic, ez=ez, embedding_function=embedding_function): \n",
    "                    res_mask.append(True)\n",
    "                else:\n",
    "                    if _validate_example(txt, db_examples, adder_metrics_threasholds=adder_metrics_threasholds, ic=ic, ez=ez, embedding_function=embedding_function): \n",
    "                        res_mask.append(True)\n",
    "                    else:\n",
    "                        res_mask.append(False)\n",
    "\n",
    "        # res_texts.append(txt)\n",
    "        # res_metadatas.append(md)\n",
    "\n",
    "    return res_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_load_from_file: gguf version     = 2\n",
      "bert_load_from_file: gguf alignment   = 32\n",
      "bert_load_from_file: gguf data offset = 695552\n",
      "bert_load_from_file: model name           = BERT\n",
      "bert_load_from_file: model architecture   = bert\n",
      "bert_load_from_file: model file type      = 1\n",
      "bert_load_from_file: bert tokenizer vocab = 30522\n"
     ]
    }
   ],
   "source": [
    "embedding_function=GPT4AllEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maciej/anaconda3/envs/ml_lm/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/maciej/anaconda3/envs/ml_lm/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/maciej/anaconda3/envs/ml_lm/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({True: 8866, False: 1760})\n"
     ]
    }
   ],
   "source": [
    "DFF = \"cbr_augmentation_12_gpt2_results_p_ix\"\n",
    "\n",
    "gen_1 = pd.read_csv(f\"{DFF}.csv\")\n",
    "gen_comp = gen_1\n",
    "comp = gen_comp.steps.to_numpy()\n",
    "\n",
    "res = validate_examples(comp, samples, metrics_threasholds, 10, 0.65, embedding_function)\n",
    "from collections import Counter\n",
    "nres = [not e for e in res]\n",
    "\n",
    "print(Counter(res))\n",
    "gen_comp[res].to_csv(f\"{DFF}_f.csv\", index=False)\n",
    "gen_comp[nres].to_csv(f\"{DFF}_nf.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counter({True: 8866, False: 1760})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFF = \"cbr_augmentation_1_gpt2_results_p\"\n",
    "\n",
    "gen_1 = pd.read_csv(f\"{DFF}.csv\")\n",
    "gen_comp = gen_1\n",
    "comp = gen_comp.result.to_numpy()\n",
    "\n",
    "res = validate_examples(comp, samples, metrics_threasholds, 10, 0.65, embedding_function)\n",
    "from collections import Counter\n",
    "nres = [not e for e in res]\n",
    "\n",
    "print(Counter(res))\n",
    "gen_comp[res].to_csv(f\"{DFF}_f.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DFF = \"cbr_augmentation_2_gpt2_results_p\"\n",
    "\n",
    "gen_1 = pd.read_csv(f\"{DFF}.csv\")\n",
    "gen_comp = gen_1\n",
    "comp = gen_comp.result.to_numpy()\n",
    "\n",
    "res2 = validate_examples(comp, samples, metrics_threasholds, 10, 0.65, embedding_function)\n",
    "from collections import Counter\n",
    "nres2 = [not e for e in res2]\n",
    "\n",
    "print(Counter(res2))\n",
    "gen_comp[res2].to_csv(f\"{DFF}_f.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
